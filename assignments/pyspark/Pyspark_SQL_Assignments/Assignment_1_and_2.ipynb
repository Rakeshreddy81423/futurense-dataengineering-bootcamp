{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e6f242",
   "metadata": {},
   "source": [
    "# Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f82e9a",
   "metadata": {},
   "source": [
    "## Assignment - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c3e479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f004e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import  *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa882a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"SparkApp\").getOrCreate()\n",
    "#.config(\"spark.jars\", \"/usr/share/java/mysql-connector-java-8.0.22.jar\")\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf99083",
   "metadata": {},
   "source": [
    "# Weather Data Analysis with DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4454bdca",
   "metadata": {},
   "source": [
    "## a) Load Weather Dataset and create DataFrame\n",
    "### *The data is seperated by one or more spaces. So first transform the data and do the calculations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b2d15fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "path  = \"D:/futurense_hadoop-pyspark/labs/dataset/weather/weather_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2f0112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['WBANNO',\n",
    " 'LST_DATE',\n",
    " 'CRX_VN',\n",
    " 'LONGITUDE',\n",
    " 'LATITUDE',\n",
    " 'T_DAILY_MAX',\n",
    " 'T_DAILY_MIN',\n",
    " 'T_DAILY_MEAN',\n",
    " 'T_DAILY_AVG',\n",
    " 'P_DAILY_CALC',\n",
    " 'SOLARAD_DAILY',\n",
    " 'SUR_TEMP_DAILY_TYPE',\n",
    " 'SUR_TEMP_DAILY_MAX',\n",
    " 'SUR_TEMP_DAILY_MIN',\n",
    " 'SUR_TEMP_DAILY_AVG',\n",
    " 'RH_DAILY_MAX',\n",
    " 'RH_DAILY_MIN',\n",
    " 'RH_DAILY_AVG',\n",
    " 'SOIL_MOISTURE_5_DAILY',\n",
    " 'SOIL_MOISTURE_10_DAILY',\n",
    " 'SOIL_MOISTURE_20_DAILY',\n",
    " 'SOIL_MOISTURE_50_DAILY',\n",
    " 'SOIL_MOISTURE_100_DAILY',\n",
    " 'SOIL_TEMP_5_DAILY',\n",
    " 'SOIL_TEMP_10_DAILY',\n",
    " 'SOIL_TEMP_20_DAILY',\n",
    " 'SOIL_TEMP_50_DAILY',\n",
    " 'SOIL_TEMP_100_DAILY']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d14c98",
   "metadata": {},
   "source": [
    "### *First read the data remove extra spaces and make it as comma seperated values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e42ac88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|23907,20150101,2....|\n",
      "|23907,20150102,2....|\n",
      "|23907,20150103,2....|\n",
      "|23907,20150104,2....|\n",
      "|23907,20150105,2....|\n",
      "|23907,20150106,2....|\n",
      "|23907,20150107,2....|\n",
      "|23907,20150108,2....|\n",
      "|23907,20150109,2....|\n",
      "|23907,20150110,2....|\n",
      "|23907,20150111,2....|\n",
      "|23907,20150112,2....|\n",
      "|23907,20150113,2....|\n",
      "|23907,20150114,2....|\n",
      "|23907,20150115,2....|\n",
      "|23907,20150116,2....|\n",
      "|23907,20150117,2....|\n",
      "|23907,20150118,2....|\n",
      "|23907,20150119,2....|\n",
      "|23907,20150120,2....|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.load(path,format=\"csv\")\n",
    "\n",
    "#df = data.withColumn(\"values\", regexp_replace(data.values, \"\\s+\", \",\")).withColumn(\"values\",split(data.value,\",\"))\n",
    "\n",
    "df = data.withColumn(\"_c0\",regexp_replace(data._c0,\"\\s+\",\",\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b8d400",
   "metadata": {},
   "source": [
    "### *Split the data based on ',' and store values in respective columns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0d2d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "split_data = df.select(split(df._c0, \",\").alias(\"data\"))\n",
    "\n",
    "\n",
    "converted_data = split_data.select(\n",
    "    *[split_data.data.getItem(idx).alias(columns[idx]) for idx in range(len(columns))])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ea8b18",
   "metadata": {},
   "source": [
    "## b) Show Min and Max Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12d6b186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|T_DAILY_MAX|T_DAILY_MIN|\n",
      "+-----------+-----------+\n",
      "|       36.0|       -7.9|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select the required the columns and type cast them into required datatype     \n",
    "converted_data.select(\n",
    "                        max(col(\"T_DAILY_MAX\").cast(FloatType())).alias(\"T_DAILY_MAX\"),\n",
    "                        min(col(\"T_DAILY_MIN\").cast(FloatType())).alias(\"T_DAILY_MIN\")\n",
    "                      ).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6b3a89",
   "metadata": {},
   "source": [
    "## c) Show month wise Min and Max Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37d76b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+\n",
      "|MONTH|Min_temp|Max_temp|\n",
      "+-----+--------+--------+\n",
      "|  May|    14.3|    31.1|\n",
      "|  Jun|     0.0|    33.6|\n",
      "|  Feb|    -3.5|    26.6|\n",
      "|  Mar|    -3.2|    29.1|\n",
      "|  Jan|    -7.9|    26.5|\n",
      "|  Apr|     8.0|    30.8|\n",
      "|  Jul|    19.8|    36.0|\n",
      "+-----+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "months = {'01':'Jan','02':'Feb','03':'Mar','04':'Apr','05':'May','06':'Jun',\n",
    "          '07':'Jul','08':'Aug','09':'Sep','10':'Oct','11':'Nov','12':'Dec'}\n",
    "\n",
    "def find_month_name(month_num):\n",
    "    return months[month_num]\n",
    "\n",
    "month_df = udf(find_month_name,StringType())\n",
    "\n",
    "\n",
    "df = converted_data.withColumn('MONTH',month_df(substring('LST_DATE',5,2)))\\\n",
    "                   .select(\n",
    "                            col('MONTH'),col(\"T_DAILY_MAX\").cast(FloatType()).alias(\"T_DAILY_MAX\"),\n",
    "                            col(\"T_DAILY_MIN\").cast(FloatType()).alias(\"T_DAILY_MIN\")\n",
    "                          )\n",
    "\n",
    "df.groupBy('MONTH').agg(min('T_DAILY_MIN').alias('Min_temp'),max('T_DAILY_MAX').alias('Max_temp')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af9ca39",
   "metadata": {},
   "source": [
    "# Assignment (Optional)\n",
    "## Find the total count of ratings for each type of rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e41efef",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:/Users/rakes/OneDrive/Desktop/ml-latest-small/ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "045b3d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|rating|count|\n",
      "+------+-----+\n",
      "|   1.0| 2811|\n",
      "|   4.5| 8551|\n",
      "|   2.5| 5550|\n",
      "|   3.5|13136|\n",
      "|   5.0|13211|\n",
      "|   0.5| 1370|\n",
      "|   4.0|26818|\n",
      "|   1.5| 1791|\n",
      "|   2.0| 7551|\n",
      "|   3.0|20047|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_df = spark.read.csv(path,header=True)\n",
    "\n",
    "ratings_df.groupBy('rating').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad6ca89",
   "metadata": {},
   "source": [
    "# Assignment-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4993a76e",
   "metadata": {},
   "source": [
    "## Bank Marketing Campaign Data Analysis with DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10d231e",
   "metadata": {},
   "source": [
    "## a) Load Bank Marketing Dataset and create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "879e6ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = 'D:/futurense_hadoop-pyspark/labs/dataset/bankmarket/bankmarketdata.csv'\n",
    "bank_df = spark.read.csv(path,header=True,sep=';',inferSchema=True)\n",
    "bank_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b33d9c",
   "metadata": {},
   "source": [
    "## b.)\tGive marketing success rate. (No. of people subscribed / total no. of entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1caabe91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 11.698480458295547\n"
     ]
    }
   ],
   "source": [
    "total_count = bank_df.count()\n",
    "\n",
    "success_rate = bank_df.filter(bank_df['y'] == 'yes').count() / total_count * 100\n",
    "\n",
    "print(\"Success rate:\",success_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bab8767e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      success_rate|\n",
      "+------------------+\n",
      "|11.698480458295547|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_df.createOrReplaceTempView('bank')\n",
    "\n",
    "spark.sql(\"select sum(if(y='yes',1,0))/count(*) * 100 as success_rate from bank\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a357a118",
   "metadata": {},
   "source": [
    "   ## c) Give marketing failure rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59b3b791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure rate: 88.30151954170445\n"
     ]
    }
   ],
   "source": [
    "failure_rate = bank_df.filter(bank_df.y == 'no').count() / total_count * 100\n",
    "\n",
    "print(\"Failure rate:\",failure_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8434342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|     failure_rate|\n",
      "+-----------------+\n",
      "|88.30151954170445|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select count(y)/(select count(*) from bank) * 100 as failure_rate from bank where y=\"no\"').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7088f697",
   "metadata": {},
   "source": [
    "## d) Maximum, Mean, and Minimum age of the average targeted customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "258ab032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|Min_Age|Max_Age|Avg_age|\n",
      "+-------+-------+-------+\n",
      "|     18|     95|  40.94|\n",
      "+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_df.select('age').agg(min(bank_df.age).alias('Min_Age'),\n",
    "                          max(bank_df.age).alias('Max_Age'),\n",
    "                          round(avg(bank_df.age),2).alias('Avg_age')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3efd9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|min_age|max_age|avg_age|\n",
      "+-------+-------+-------+\n",
      "|     18|     95|   40.9|\n",
      "+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select min(age) as min_age,max(age) as max_age, round(avg(age),1) as avg_age from bank\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661bdcdc",
   "metadata": {},
   "source": [
    "## e.)\tCheck the quality of customers by checking the average balance, median balance of customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64525e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|Avg_balance|Median_balance|\n",
      "+-----------+--------------+\n",
      "|    1362.27|           448|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_df.agg(\n",
    "            round(avg(bank_df.balance),2).alias(\"Avg_balance\"),\n",
    "            percentile_approx(bank_df.balance,0.5).alias(\"Median_balance\")\n",
    "            ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3db2649e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|avg_balance|median_balance|\n",
      "+-----------+--------------+\n",
      "|    1362.27|           448|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select round(avg(balance),2) as avg_balance, percentile_approx(balance,0.5) as median_balance from bank\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b1fe55",
   "metadata": {},
   "source": [
    "## f.)\tCheck if age matters in marketing subscription for deposit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d3d559e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|Age_group|count|\n",
      "+---------+-----+\n",
      "|      61+|  502|\n",
      "|     0-20|   33|\n",
      "|    21-30| 1112|\n",
      "|    41-50| 1019|\n",
      "|    31-40| 1812|\n",
      "|    51-60|  811|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_df.filter(col('y') == 'yes').select('age').withColumn('Age_group', when(col('age') <= 20 ,'0-20')\\\n",
    "                                            .when((col('age') > 20) & (col('age') <= 30) , '21-30')\\\n",
    "                                             .when((col('age') > 30) & (col('age') <= 40) , '31-40')\\\n",
    "                                             .when((col('age') > 40) & (col('age') <= 50) , '41-50')\\\n",
    "                                             .when((col('age') > 50) & (col('age') <= 60) , '51-60')\\\n",
    "                                            .otherwise('61+')\\\n",
    "                                ).groupBy('Age_group').count().show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "15b3344a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|age_group|count|\n",
      "+---------+-----+\n",
      "|     0-20|   33|\n",
      "|      60+|  502|\n",
      "|    21-30| 1112|\n",
      "|    41-50| 1019|\n",
      "|    31-40| 1812|\n",
      "|    51-60|  811|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bank_df.createOrReplaceTempView('bank')\n",
    "\n",
    "\n",
    "spark.sql(\"select age_group, count(age) as count from \\\n",
    "          (select age,case when age<=20 then '0-20' \\\n",
    "                when (age>=21 and age<=30) then '21-30'\\\n",
    "                when (age>=31 and age<=40) then '31-40' \\\n",
    "                when (age>=41 and age<=50) then '41-50' \\\n",
    "                when (age>=51 and age<=60) then '51-60' \\\n",
    "                else '60+' end as age_group \\\n",
    "               from bank where y = 'yes') a group by age_group\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4f8849",
   "metadata": {},
   "source": [
    "## g) Show AgeGroup [Teenagers, Youngsters, MiddleAgers, Seniors] wise Subscription Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c92a2e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|  Age_group|count|\n",
      "+-----------+-----+\n",
      "| Youngsters| 2924|\n",
      "|    Seniors|  502|\n",
      "|  Teenagers|   33|\n",
      "|MiddleAgers| 1830|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_df = bank_df.filter(col('y')=='yes').select('age')\\\n",
    "       .withColumn('Age_group',when(col('age') <= 20,'Teenagers')\n",
    "                               .when((col('age') >= 21) & (col('age') <= 40) ,'Youngsters')\n",
    "                               .when((col('age') >= 41) & (col('age') <= 60) ,'MiddleAgers')\n",
    "                               .otherwise('Seniors')\n",
    "                  )\n",
    "\n",
    "grouped_df.groupBy('Age_group').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "35ad475f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|  age_group|count|\n",
      "+-----------+-----+\n",
      "| Youngsters| 2924|\n",
      "|    Seniors|  502|\n",
      "|  Teenagers|   33|\n",
      "|MiddleAgers| 1830|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select age_group, count(age) as count from (select age, case when age<=20 then 'Teenagers'\\\n",
    "                            when (age >= 21 and age <= 40) then 'Youngsters'\\\n",
    "                            when (age >= 41 and age <=60) then 'MiddleAgers'\\\n",
    "                            else 'Seniors' end as age_group from bank where y = 'yes') a group by age_group\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793feedf",
   "metadata": {},
   "source": [
    "## h) Check if marital status mattered for subscription to deposit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ecfd227f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "| marital|count|\n",
      "+--------+-----+\n",
      "|divorced|  622|\n",
      "| married| 2755|\n",
      "|  single| 1912|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_df.select('marital','y').filter(bank_df.y == 'yes').groupBy('marital').count().show()                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "91803c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "| marital|count|\n",
      "+--------+-----+\n",
      "|divorced|  622|\n",
      "| married| 2755|\n",
      "|  single| 1912|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select marital, count(y) as count from bank where y='yes' group by marital\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffeb511",
   "metadata": {},
   "source": [
    "## i) Check if age and marital status together mattered for subscription to deposit scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91799665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "|age| marital|count|\n",
      "+---+--------+-----+\n",
      "| 18|  single|    7|\n",
      "| 19|  single|   11|\n",
      "| 20|  single|   14|\n",
      "| 20| married|    1|\n",
      "| 21|  single|   21|\n",
      "| 21| married|    1|\n",
      "| 22|  single|   40|\n",
      "| 23| married|    2|\n",
      "| 23|  single|   42|\n",
      "| 24|  single|   58|\n",
      "| 24| married|   10|\n",
      "| 25| married|   14|\n",
      "| 25|  single|   99|\n",
      "| 26| married|   13|\n",
      "| 26|  single|  121|\n",
      "| 27| married|   29|\n",
      "| 27|  single|  110|\n",
      "| 27|divorced|    2|\n",
      "| 28| married|   20|\n",
      "| 28|  single|  138|\n",
      "+---+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_df.select('age','marital','y')\\\n",
    "        .filter(bank_df.y == 'yes').groupBy('age','marital')\\\n",
    "        .count().orderBy('age').show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "87aa7eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "|age| marital|count|\n",
      "+---+--------+-----+\n",
      "| 42|  single|   22|\n",
      "| 66| married|   22|\n",
      "| 68|  single|    2|\n",
      "| 28| married|   20|\n",
      "| 59| married|   66|\n",
      "| 61| married|   47|\n",
      "| 21|  single|   21|\n",
      "| 29|  single|  133|\n",
      "| 70|divorced|    5|\n",
      "| 56|  single|    6|\n",
      "| 74| married|   11|\n",
      "| 64|divorced|    4|\n",
      "| 40|  single|   31|\n",
      "| 45|  single|   14|\n",
      "| 83|  single|    1|\n",
      "| 69| married|   13|\n",
      "| 77| married|   19|\n",
      "| 57|divorced|   15|\n",
      "| 21| married|    1|\n",
      "| 53| married|   60|\n",
      "+---+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select age,marital,count(y) as count from bank where y='yes' group by age,marital\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67830423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
